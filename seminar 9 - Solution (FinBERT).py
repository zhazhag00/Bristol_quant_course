# -*- coding: utf-8 -*-
"""Seminar 9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1puNn202ZGVMYBeU8xJ4uCWDWNGPlx7zL
"""

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# set your data path here
PATH_2_data = '/content/gdrive/MyDrive/QMBDML/'

"""
!pip install transformers
!pip install spacy_universal_sentence_encoder
!pip install tqdm
"""

import pandas as pd
import ast
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import BertTokenizer, BertForSequenceClassification
from tqdm import tqdm

# Load the FinBERT model and tokenizer
model_name = "yiyanghkust/finbert-tone"
tokenizer = AutoTokenizer.from_pretrained(model_name,num_labels=3)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Set the device to GPU if available, else use CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def analyze_sentiment_batch(texts, model, tokenizer, device):

    # Tokenize the input texts
    tokens = tokenizer.batch_encode_plus(texts, max_length=512, truncation=True,\
                                         padding="max_length", return_tensors="pt")
    tokens = {k: v.to(device) for k, v in tokens.items()}

    # Forward pass through the model
    with torch.no_grad():
        outputs = model(**tokens)

    # Get the predicted sentiment labels
    predicted_labels = torch.argmax(outputs.logits, dim=1)

    sentiments = []
    for label in predicted_labels:
        if label == 0:
            sentiment = "Neutral"
        elif label == 1:
            sentiment = "Positive"
        elif label == 2:
            sentiment = "Negative"
        sentiments.append(sentiment)

    return sentiments

# load data
wsj = pd.read_csv(PATH_2_data + "wsj.csv")
wsj['sentences'] = wsj['sentences'].apply(lambda x: ast.literal_eval(x) )

'''
import nltk
nltk.download('punkt')  # Download the necessary data
from nltk.tokenize import sent_tokenize

text = "This is a sample text. It contains multiple sentences. Let's split it into sentences."

sentences = sent_tokenize(text)

for sentence in sentences:
    print(sentence)
'''
    

#for _, row in wsj.iterrows():
for _, row in tqdm(wsj.iterrows(), total=wsj.shape[0]):
    print(row.name)

    # calculate the sentence-level tone
    row['FinBert_sent'] = analyze_sentiment_batch(row['sentences'], model, tokenizer, device)

    row['FinBert_sent_neg'] = row['FinBert_sent'].count('Negative')
    row['FinBert_sent_neu'] = row['FinBert_sent'].count('Neutral')
    row['FinBert_sent_pos'] = row['FinBert_sent'].count('Positive')

    row.to_csv(PATH_2_data + "FinBERT_result_%s.csv"%row.name,index=False)
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    